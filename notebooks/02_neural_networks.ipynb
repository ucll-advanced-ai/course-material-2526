{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: A Foundational Introduction\n",
        "\n",
        "This lecture introduces neural networks, focusing on the fundamental concepts and building blocks.  We'll cover perceptrons, multi-layer perceptrons (MLPs), activation functions, backpropagation, and training. This foundation will be crucial for understanding more advanced topics like LLMs, Generative AI, Computer Vision, and Reinforcement Learning.\n",
        "\n",
        "**Prerequisites:** Familiarity with machine learning and supervised learning concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction and Motivation\n",
        "\n",
        "*   **Recap of supervised learning**\n",
        "*   **Neural networks** \n",
        "*   **Deep Learning Revolution**\n",
        "*   **Future Topics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning Review\n",
        "\n",
        "As you already know, supervised learning is a fundamental branch of machine learning where we train a model on labeled data, meaning data with both inputs and desired outputs.  Our goal is to learn a mapping function that can accurately predict the output for new, unseen inputs. We've explored various algorithms like linear regression, logistic regression, support vector machines, and decision trees.  These methods have proven useful for many tasks, but they often rely heavily on feature engineering. This means we, as humans, need to carefully craft and select the right features from the raw data to feed into the model.  This process can be time-consuming, require domain expertise, and it's not always clear which features will be most effective. Furthermore, some algorithms struggle with highly complex, non-linear relationships in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Networks\n",
        "\n",
        "Now, let's turn our attention to neural networks.  Neural networks offer a powerful and flexible alternative. They are inspired by the structure and function of the human brain, although they are, of course, vastly simplified. At their core, neural networks are function approximators.  Given some input, they learn to produce an output.  But unlike the algorithms we've seen before, neural networks have the remarkable ability to learn complex, non-linear relationships directly from the data without the need for explicit feature engineering.  They achieve this through interconnected layers of artificial neurons, allowing them to automatically discover and extract relevant features. This ability to learn hierarchical representations makes them incredibly versatile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learning\n",
        "Over the past decade, we've witnessed what's often called the 'deep learning revolution.' Deep learning, which refers to neural networks with multiple layers (hence 'deep'), has achieved groundbreaking results in a wide range of fields. Think about image recognition: self-driving cars, facial recognition, medical image analysis – all powered by deep learning. Natural language processing has also seen tremendous progress. We now have sophisticated chatbots, machine translation systems, and sentiment analysis tools, all thanks to deep learning. These are just a couple of examples.  Deep learning is transforming fields like robotics, drug discovery, finance, and many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Future Topics\n",
        "\n",
        "The power of neural networks extends far beyond the examples I just mentioned. And, importantly for you, they form the bedrock for many of the topics you'll be exploring later in this course. Large Language Models (LLMs), like the ones powering advanced chatbots, are built upon neural network architectures.\n",
        "\n",
        "Generative AI (GenAI), which allows us to create realistic images, text, and even music, relies heavily on specialized neural networks.  Computer vision, the field that enables computers to 'see' and interpret images, uses convolutional neural networks. And even in reinforcement learning, where agents learn to make decisions through trial and error, neural networks are often used to approximate the optimal policy. So, understanding the fundamentals of neural networks that we'll cover today is absolutely essential for your future studies in these cutting-edge areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptron and Multi-Layer Perceptron (MLP)\n",
        "\n",
        "A perceptron is the simplest unit of a neural network. It takes multiple inputs, each multiplied by a weight, and sums them up. This sum is then passed through an activation function to produce an output. Think of it like a single neuron in your brain making a decision based on the signals it receives.\n",
        "\n",
        "Let's break down its components:\n",
        "\n",
        "* Inputs: These are the initial pieces of information fed into the perceptron, analogous to the signals a biological neuron receives through its dendrites.\n",
        "\n",
        "* Weights: Each input is associated with a weight, which represents the strength or importance of that input. Weights are crucial in determining how much each input influences the final output.\n",
        "\n",
        "* Summation: The perceptron calculates a weighted sum of its inputs, multiplying each input by its corresponding weight and adding them together.\n",
        "\n",
        "* Bias: A bias term is added to the weighted sum. This bias acts as an offset, allowing the perceptron to shift the activation function and make more flexible decisions.\n",
        "\n",
        "* Activation Function: The result of the summation and bias is passed through an activation function. This function introduces non-linearity, enabling the perceptron to learn complex patterns. Common activation functions include the sigmoid function, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent).\n",
        "\n",
        "* Output: The output of the activation function is the final result of the perceptron's computation. This output can be used for various tasks, such as classification or regression.\n",
        "\n",
        "![perceptron](images/perceptron.png)\n",
        "\n",
        "Now, a single perceptron has limitations in its learning capacity. It can only solve **linearly** separable problems, meaning problems where the data points can be perfectly separated by a single line or hyperplane. However, most real-world problems are not linearly separable. To overcome this limitation, we introduce the Multi-Layer Perceptron (MLP).\n",
        "\n",
        "MLPs consist of multiple layers of perceptrons, organized in an interconnected structure. MLPs overcome the limitations of single perceptrons by adding hidden layers between the input and output layers. These hidden layers allow the network to learn non-linear relationships. Each neuron in a hidden layer performs a weighted sum of its inputs, passes it through an activation function, and sends the output to the next layer. These layers include:\n",
        "\n",
        "* Input Layer: This layer receives the initial inputs to the network.\n",
        "\n",
        "* Hidden Layers: These are intermediate layers between the input and output layers. Each hidden layer contains multiple perceptrons that process the information received from the previous layer and pass their outputs to the next layer. Hidden layers enable the network to learn complex, non-linear relationships in the data.\n",
        "\n",
        "* Output Layer: This layer produces the final output of the network. The number of neurons in the output layer depends on the specific task. For example, in a binary classification problem, there would be one output neuron, while a multi-class classification problem might have multiple output neurons.\n",
        "\n",
        "![multi-layer-perceptron](images/multi-layer-perceptron.png)\n",
        "\n",
        "MLPs, with their multiple layers and non-linear activation functions, can approximate complex decision boundaries and solve problems that are not linearly separable. They are the foundation for many advanced neural network architectures and have revolutionized various fields, including image recognition, natural language processing, and robotics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Functions and Backpropagation\n",
        "\n",
        "### Activation Functions\n",
        "Activation functions are crucial components of neural networks. They introduce non-linearity, enabling the network to learn complex patterns and relationships in data. Let's explore some common activation functions:\n",
        "\n",
        "* Sigmoid: The sigmoid function squashes the input to a range between 0 and 1. It's often used in binary classification problems, where the output represents the probability of belonging to a certain class. However, it suffers from the vanishing gradient problem, where gradients become very small during backpropagation, hindering learning in deep networks.   \n",
        "* Tanh (Hyperbolic Tangent): Similar to the sigmoid function, tanh squashes the input, but to a range between -1 and 1. It often performs better than sigmoid in practice, as it centers the output around 0, which can help with optimization. However, it also suffers from the vanishing gradient problem.\n",
        "* ReLU (Rectified Linear Unit): ReLU is a popular activation function that returns the input if it's positive, otherwise 0. It's computationally efficient and often leads to faster training. It also mitigates the vanishing gradient problem to some extent. However, it can suffer from the \"dying ReLU\" problem, where neurons get stuck at 0 and stop learning.\n",
        "* Leaky ReLU: Leaky ReLU is a variant of ReLU that introduces a small slope for negative inputs, preventing the dying ReLU problem. It often performs better than ReLU in practice.\n",
        "\n",
        "\n",
        "Why are ReLU-based activations often preferred?\n",
        "\n",
        "* ReLU and its variants are often preferred due to their computational efficiency and ability to mitigate the vanishing gradient problem. They generally lead to faster training and better performance in deep networks.\n",
        "\n",
        "### Backpropagation\n",
        "Backpropagation is the key algorithm for training neural networks. It allows us to efficiently calculate the gradients of the network's parameters with respect to the loss function, enabling us to update the parameters and improve the network's predictions.\n",
        "\n",
        "* Chain Rule of Calculus: Backpropagation utilizes the chain rule of calculus to compute gradients. The chain rule allows us to break down the computation of complex derivatives into smaller, more manageable steps.\n",
        "\n",
        "* Gradient Descent: The gradients calculated through backpropagation are used in gradient descent, an iterative optimization algorithm that aims to find the minimum of the loss function. The gradients indicate the direction of the steepest ascent of the loss function, and by taking steps in the opposite direction (negative gradient), we can gradually descend towards the minimum, improving the network's performance.\n",
        "\n",
        "![gradient-descent](images/gradient-descent.png)\n",
        "\n",
        "### Loss Function\n",
        "The loss function quantifies the error between the network's predictions and the actual target values. It guides the optimization process by providing a measure of how well the network is performing.\n",
        "\n",
        "* Mean Squared Error (MSE): MSE is a common loss function for regression problems. It calculates the average squared difference between the predicted and actual values.\n",
        "\n",
        "* Cross-Entropy: Cross-entropy is a popular loss function for classification problems. It measures the dissimilarity between the predicted probability distribution and the true distribution of the classes.   \n",
        "\n",
        "The choice of loss function depends on the specific learning task. The goal of backpropagation and gradient descent is to minimize the chosen loss function, leading to improved predictions and better performance on the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Neural Network\n",
        "\n",
        "Training a neural network involves feeding it data, adjusting its parameters to improve its predictions, and monitoring its performance. Let's explore the key concepts involved in this process:\n",
        "\n",
        "### Training Process\n",
        "* Epochs: An epoch refers to one complete pass through the entire training dataset. During each epoch, the network sees all the training examples and updates its parameters based on the errors it makes.\n",
        "\n",
        "* Batch Size: Instead of updating the parameters after every single training example, we often use batches of examples. The batch size determines how many examples are processed before updating the parameters. Smaller batch sizes can lead to more frequent updates and potentially faster convergence, but they can also introduce more noise in the training process. Larger batch sizes can be more computationally efficient but might require more memory.\n",
        "\n",
        "* Learning Rate: The learning rate controls the step size taken during gradient descent. It determines how much the parameters are adjusted based on the calculated gradients. A smaller learning rate leads to slower but potentially more stable learning, while a larger learning rate can speed up training but might risk overshooting the optimal solution.\n",
        "\n",
        "### Optimization Algorithms\n",
        "Optimization algorithms are used to update the network's parameters based on the calculated gradients. Some common algorithms include:\n",
        "\n",
        "* Stochastic Gradient Descent (SGD): SGD updates the parameters based on the gradient calculated from a single training example or a small batch of examples. It's a simple but widely used algorithm.\n",
        "\n",
        "* Adam: Adam (Adaptive Moment Estimation) is a popular optimization algorithm that combines the benefits of momentum and adaptive learning rates. It often converges faster and performs better than SGD in practice, making it a common choice for many deep learning tasks.\n",
        "\n",
        "    * Adam is often preferred due to its ability to automatically adjust learning rates for different parameters and its generally good performance across various tasks. It's relatively easy to use and often requires less hyperparameter tuning compared to other algorithms.\n",
        "\n",
        "### Overfitting/Underfitting\n",
        "* Overfitting: Overfitting occurs when the network learns the training data too well, capturing noise and irrelevant details. This leads to poor generalization performance on unseen data.\n",
        "\n",
        "* Underfitting: Underfitting happens when the network is too simple to capture the underlying patterns in the data. This results in poor performance on both training and unseen data.\n",
        "\n",
        "#### Addressing overfitting and underfitting:\n",
        "\n",
        "* Regularization: Techniques like L1 or L2 regularization add penalties to the loss function, discouraging the network from learning overly complex patterns.\n",
        "\n",
        "* Dropout: Dropout randomly drops out neurons during training, forcing the network to learn more robust features.\n",
        "\n",
        "* Early Stopping: Early stopping monitors the performance on a validation set and stops training when the performance starts to degrade, preventing the network from overfitting to the training data.\n",
        "\n",
        "* Validation Sets: A validation set is a portion of the data held out from training, used to evaluate the network's performance during training and to tune hyperparameters.\n",
        "\n",
        "\n",
        "### Visualization\n",
        "Visualizing the training process can provide insights into how the network is learning. Common visualizations include:\n",
        "\n",
        "* Loss Curve: Plotting the loss function over epochs can show how the error is decreasing during training.\n",
        "\n",
        "* Accuracy Curve: For classification tasks, plotting the accuracy on the training and validation sets can show how well the network is learning and generalizing.\n",
        "\n",
        "These visualizations help monitor the training progress, identify potential issues like overfitting or underfitting, and guide decisions about hyperparameter tuning and early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Practical Example: Student Placement Classification\n",
        "\n",
        "Let's try it out with a small example.\n",
        "\n",
        "We've provided a dataset which contains information about the students academic and training and placement status.\n",
        "\n",
        "Here are the columns in the dataset:\n",
        "\n",
        "* CGPA - cumulative grade point average achieved by the student\n",
        "* Internships - number of internships a student has done\n",
        "* Projects - number of projects a student has done\n",
        "* Workshops/Certifications - number of online skills courses completed by the student\n",
        "* ApptitudeTestScore - aptitude test scores to measure the student's quantitative and logical thinking\n",
        "* SoftSkillrating - a rating of the student's communication skills\n",
        "* ExtracurricularActivities - binary (Yes/No) value indicating a student's participation in non-academic activities\n",
        "* PlacementTraining - binary (Yes/No) value indicating a student's participation in placement training\n",
        "* SSC_marks - score in senior secondary school out of 100\n",
        "* HSC_marks - score in higher secondary school out of 100\n",
        "* PlacementStatus - target variable: placed or not placed (in a job or internship)\n",
        "\n",
        "#### Process:\n",
        "1. Check Device Compatibility\n",
        "Determine whether the system has an NVIDIA GPU, AMD GPU, Apple M-series chip, or just a CPU. This helps optimize training performance.\n",
        "\n",
        "2. Load and Preprocess the Data\n",
        "    * Read the dataset into a DataFrame.\n",
        "    * Convert categorical or binary values into numerical format (e.g., mapping T/F to 1/0).\n",
        "    * Encode the target variable if necessary.\n",
        "    * Split the data into training and test sets.\n",
        "    * Normalize (scale) the feature values so that different numerical ranges don't affect training.\n",
        "3. Convert Data to PyTorch Tensors\n",
        "    * Convert the feature data (X) and target labels (y) into tensors.\n",
        "    * Move the tensors to the appropriate device (CPU or GPU).\n",
        "4. Define the Neural Network Model\n",
        "    * Create a class for the model that inherits from nn.Module.\n",
        "    * Define the layers:\n",
        "        * An input layer matching the number of features.\n",
        "        * One or more hidden layers with activation functions (e.g., ReLU).\n",
        "        * An output layer that provides a probability (using Sigmoid for binary classification).\n",
        "    * Implement the forward method to define how data flows through the layers.\n",
        "5. Set Up Loss Function and Optimizer\n",
        "    * Use Binary Cross-Entropy (BCELoss) for binary classification.\n",
        "    * Use an optimizer like Adam to adjust the model weights.\n",
        "6. Train the Model\n",
        "    * Iterate over multiple epochs:\n",
        "        * Pass the training data through the model.\n",
        "        * Compute the loss.\n",
        "        * Perform backpropagation to update weights.\n",
        "        * Print loss periodically to monitor progress.\n",
        "7. Evaluate the Model\n",
        "    * Switch the model to evaluation mode.\n",
        "    * Make predictions on the test set.\n",
        "    * Round the predictions (since outputs are probabilities).\n",
        "    * Compare predictions with actual values to compute accuracy.\n",
        "8. Compare Performance on Different Hardware\n",
        "    * Run the script on different devices (CPU, NVIDIA GPU, AMD GPU).\n",
        "    * Measure training speed and accuracy.\n",
        "    * Observe the impact of hardware on training efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Information:\n",
            "Python version: 3.13.12 (tags/v3.13.12:1cbe481, Feb  3 2026, 18:22:25) [MSC v.1944 64 bit (AMD64)]\n",
            "PyTorch version: 2.10.0+cpu\n",
            "CUDA available: False\n",
            "\n",
            "CUDA Details:\n",
            "CUDA version: None\n",
            "Number of CUDA devices: 0\n",
            "\n",
            "Environment Checks:\n",
            "CUDA_HOME: Not set\n",
            "PATH environment variable contains CUDA paths: False\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "print(\"System Information:\")\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "try:\n",
        "    print(\"\\nCUDA Details:\")\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"\\nDevice {i} Details:\")\n",
        "            print(\"Device name:\", torch.cuda.get_device_name(i))\n",
        "            print(\"Device properties:\", torch.cuda.get_device_properties(i))\n",
        "except Exception as e:\n",
        "    print(\"Error retrieving CUDA information:\", str(e))\n",
        "\n",
        "print(\"\\nEnvironment Checks:\")\n",
        "import os\n",
        "print(\"CUDA_HOME:\", os.environ.get('CUDA_HOME', 'Not set'))\n",
        "print(\"PATH environment variable contains CUDA paths:\", \n",
        "      any('cuda' in path.lower() for path in os.environ.get('PATH', '').split(os.pathsep)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Use this code snippet to assign the model and data to your GPU:\n",
        "\n",
        "```python\n",
        "model.to(device)  # Move the model to the GPU\n",
        "\n",
        "# Inside the training loop:\n",
        "for batch_X, batch_y in train_loader:\n",
        "    batch_X = batch_X.to(device)  # Move the input data to the GPU\n",
        "    batch_y = batch_y.to(device)  # Move the target data to the GPU\n",
        "    #... rest of the training code...\n",
        "```\n",
        "\n",
        "* Compare the results of a simple computation using the GPU versus CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor Operations on GPU:\n",
            "X tensor: tensor([[0.3828, 0.9785, 0.3112],\n",
            "        [0.8653, 0.9674, 0.7864],\n",
            "        [0.3618, 0.5137, 0.1696],\n",
            "        [0.7675, 0.7594, 0.0391],\n",
            "        [0.6227, 0.1132, 0.4307]])\n",
            "Y tensor: tensor([[0.9021, 0.0252, 0.0530],\n",
            "        [0.2370, 0.8262, 0.7015],\n",
            "        [0.8932, 0.0983, 0.5134],\n",
            "        [0.7827, 0.9393, 0.3832],\n",
            "        [0.7646, 0.6536, 0.1527]])\n",
            "\n",
            "Addition Result: tensor([[1.2849, 1.0037, 0.3642],\n",
            "        [1.1023, 1.7936, 1.4879],\n",
            "        [1.2550, 0.6120, 0.6830],\n",
            "        [1.5502, 1.6988, 0.4224],\n",
            "        [1.3873, 0.7668, 0.5834]])\n",
            "\n",
            "Performance Comparison:\n",
            "CPU Computation Time: 1.9320635795593262\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPerformance Comparison:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCPU Computation Time:\u001b[39m\u001b[33m\"\u001b[39m, cpu_computation())\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPU Computation Time:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mgpu_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgpu_computation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgpu_computation\u001b[39m():\n\u001b[32m     28\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     x_gpu = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m     31\u001b[39m         x_gpu = torch.matmul(x_gpu, x_gpu)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\u0113512\\Documents\\advanced_ai\\course-material-2526\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:417\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    414\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    415\u001b[39m     )\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m     )\n",
            "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a random tensor on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.rand(5, 3, device=device)\n",
        "y = torch.rand(5, 3, device=device)\n",
        "\n",
        "print(\"Tensor Operations on GPU:\")\n",
        "print(\"X tensor:\", x)\n",
        "print(\"Y tensor:\", y)\n",
        "\n",
        "# Perform a simple GPU computation\n",
        "z = x + y\n",
        "print(\"\\nAddition Result:\", z)\n",
        "\n",
        "# Measure computation speed\n",
        "import time\n",
        "\n",
        "def cpu_computation():\n",
        "    start = time.time()\n",
        "    x_cpu = torch.rand(1000, 1000)\n",
        "    for _ in range(100):\n",
        "        x_cpu = torch.matmul(x_cpu, x_cpu)\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "def gpu_computation():\n",
        "    start = time.time()\n",
        "    x_gpu = torch.rand(1000, 1000, device='cuda')\n",
        "    for _ in range(100):\n",
        "        x_gpu = torch.matmul(x_gpu, x_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"CPU Computation Time:\", cpu_computation())\n",
        "print(\"GPU Computation Time:\", gpu_computation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Example: Neural Network Classification\n",
        "\n",
        "The code below trains and evaluates a neural network classifier on the placement dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU (training may be slower)\n",
            "Epoch [10/50], Loss: 0.5506\n",
            "Epoch [20/50], Loss: 0.4619\n",
            "Epoch [30/50], Loss: 0.4512\n",
            "Epoch [40/50], Loss: 0.4378\n",
            "Epoch [50/50], Loss: 0.4330\n",
            "Training complete!\n",
            "Accuracy: 79.55%\n",
            "Run this on different devices to compare training speed and performance.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Check GPU availability\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device_name = torch.cuda.get_device_name(0).lower()\n",
        "        if 'nvidia' in device_name:\n",
        "            print(\"Using NVIDIA GPU\")\n",
        "        elif 'amd' in device_name:\n",
        "            print(\"Using AMD GPU\")\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        print(\"Using Apple M1/M2 GPU\")\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        print(\"Using CPU (training may be slower)\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"data/placementdata.csv\")\n",
        "\n",
        "# Convert binary columns\n",
        "binary_columns = ['ExtracurricularActivities', 'PlacementTraining']\n",
        "for col in binary_columns:\n",
        "    data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "data['PlacementStatus'] = label_encoder.fit_transform(data['PlacementStatus'])\n",
        "\n",
        "# Select features and target\n",
        "X = data.drop(columns=['PlacementStatus'])\n",
        "y = data['PlacementStatus']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Define neural network class\n",
        "class PlacementClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PlacementClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # this is the forward pass of the neural network: it takes the input tensor x, applies a linear transformation followed by a ReLU activation function, then another linear transformation and ReLU, and finally a linear transformation followed by a sigmoid activation function to produce the output.\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = PlacementClassifier().to(device) # this line initializes an instance of the PlacementClassifier class and moves it to the specified device (GPU or CPU) for training. It's important to move the model to the same device as the input tensors to ensure that computations are performed correctly and efficiently.\n",
        "criterion = nn.BCELoss() # this line initializes the binary cross-entropy loss function, which is commonly used for binary classification tasks. It measures the difference between the predicted probabilities and the actual binary labels, providing a way to evaluate how well the model is performing during training.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam optimizer is an optimization algorithm that adjusts the learning rate for each parameter based on the first and second moments of the gradients. In this line, we initialize the Adam optimizer with the model's parameters and set a learning rate of 0.01, which controls how much the model's weights are updated during training. The Adam optimizer is often preferred for its efficiency and ability to handle sparse gradients, making it a good choice for training neural networks.\n",
        "\n",
        "# Training loop \n",
        "# in PyTorch, you have to manually implement the training loop, which involves iterating over the dataset, performing forward passes to compute predictions, calculating the loss, performing backpropagation to compute gradients, and updating the model's parameters using the optimizer. This gives you more control over the training process and allows for customization, but it also requires more code compared to high-level libraries that abstract away these details.\n",
        "num_epochs = 50 # number of times you want to iterate over the whole training dataset during the training process. Setting it to 50 means that the model will see the entire training data 50 times\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # we tell the model that we are training = necessary for applying dropout and batch normalization correctly during training. It sets the model to training mode, which allows certain layers to behave differently than during evaluation (e.g., dropout will randomly zero out some activations during training but not during evaluation).\n",
        "    optimizer.zero_grad() # this line resets the gradients of all model parameters to zero before starting the backpropagation step. In PyTorch, gradients are accumulated by default, so if you don't reset them, they will be added to the existing gradients from previous iterations, which can lead to incorrect updates. By calling optimizer.zero_grad(), you ensure that each training iteration starts with a clean slate for gradient computation.\n",
        "    outputs = model(X_train_tensor).squeeze() # this line performs a forward pass through the model using the training data (X_train_tensor) and computes the predicted outputs. The .squeeze() method is used to remove any extra dimensions from the output tensor, which is necessary because the model's final layer produces a single value for each input sample (binary classification), and we want to ensure that the output shape matches the shape of the target labels (y_train_tensor) for loss calculation.\n",
        "    loss = criterion(outputs, y_train_tensor) # after the forward pass, we have to calculate the loss, which is a measure of how well the model's predictions match the actual target labels. In this line, we use the criterion (binary cross-entropy loss) to compute the loss between the predicted outputs and the true labels (y_train_tensor). This loss value will be used to guide the optimization process during backpropagation.\n",
        "    loss.backward() # finally, we call loss.backward() to perform backpropagation, which computes the gradients of the loss with respect to each model parameter. This step is crucial for updating the model's weights in the direction that minimizes the loss. After calling backward(), the gradients are stored in the .grad attribute of each parameter, and we can then use these gradients to update the parameters using the optimizer.\n",
        "    optimizer.step() # this line updates the model's parameters based on the computed gradients. After calling loss.backward(), the gradients are stored in the .grad attribute of each parameter. By calling optimizer.step(), we apply these gradients to update the parameters according to the optimization algorithm (in this case, Adam) and the specified learning rate. This step is what allows the model to learn from the training data and improve its performance over time.\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate model\n",
        "model.eval() # As we are not training anymore, we have to tell the model that it's in evaluation mode. This is important because certain layers like dropout and batch normalization behave differently during training and evaluation. By calling model.eval(), we ensure that these layers operate in evaluation mode, which means that dropout will not randomly zero out activations and batch normalization will use running statistics instead of batch statistics. This allows us to get accurate predictions from the model when evaluating its performance on the test set.\n",
        "with torch.no_grad(): # since we are not training, we can wrap the evaluation code in a torch.no_grad() context to disable gradient calculations. This is important for memory efficiency and speed, as we don't need to compute gradients during evaluation. By using torch.no_grad(), we prevent PyTorch from tracking operations for gradient computation, which can save memory and improve performance when making predictions.\n",
        "    y_pred = model(X_test_tensor).squeeze().round()\n",
        "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy.item()*100:.2f}%')\n",
        "\n",
        "print(\"Run this on different devices to compare training speed and performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps:\n",
        "\n",
        "There are several ways you can experiment with this first model to better understand deep learning concepts and improve performance. \n",
        "Here are some suggestions:\n",
        "\n",
        "1. Adjust Model Architecture\n",
        "    * Increase or decrease hidden layers: Try adding more layers or making the model shallower.\n",
        "    * Change the number of neurons per layer: Test different values, such as 32, 64, or 128, to see how it affects accuracy.\n",
        "    * Try different activation functions: Replace ReLU with LeakyReLU, Tanh, or Sigmoid and compare results.\n",
        "2. Experiment with Hyperparameters\n",
        "    * Learning Rate: Try lowering it (e.g., 0.001) or increasing it (e.g., 0.1) to observe changes in convergence.\n",
        "    * Batch Size: Use mini-batch training instead of full-batch gradient descent.\n",
        "    * Optimizer Choice: Compare Adam with SGD, RMSprop, or Adagrad to see how training speed and accuracy change.\n",
        "3. Feature Engineering & Data Processing\n",
        "    * Try feature selection: Remove certain columns and check if accuracy improves or declines.\n",
        "    * Handle categorical data differently: Instead of mapping T/F to 1/0, try one-hot encoding.\n",
        "    * Experiment with different normalization techniques: Try Min-Max scaling instead of Standardization.\n",
        "4. Modify Training Strategy\n",
        "    * Increase or decrease the number of epochs: Does training for 100 epochs improve accuracy, or does it overfit?\n",
        "    * Use dropout layers: Add nn.Dropout() to prevent overfitting and observe the difference.\n",
        "    * Implement early stopping: Stop training automatically if the validation loss stops improving.\n",
        "5. Evaluate Performance in Different Ways\n",
        "    * Confusion Matrix: Visualize true positives, false positives, etc., instead of just accuracy.\n",
        "    * Precision, Recall, and F1-score: Compute these metrics to better understand model performance.\n",
        "    * Cross-validation: Instead of a single train-test split, use K-Fold cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing Appropriate Hyperparameters (Hyperparameter Tuning)\n",
        "Hyperparameters are settings that control the learning process of a neural network, such as the learning rate, batch size, number of hidden layers, and number of neurons per layer. Choosing appropriate hyperparameters is crucial for optimal performance.   \n",
        "\n",
        "* Manual Tuning: You can manually adjust hyperparameters based on your understanding of the model and the data. For example, if the model is overfitting, you might try reducing the learning rate or adding regularization.   \n",
        "\n",
        "* Grid Search: Grid search involves defining a set of possible values for each hyperparameter and trying all possible combinations. This can be computationally expensive but can help find a good set of hyperparameters.   \n",
        "\n",
        "* Random Search: Random search randomly samples hyperparameter values from a defined range. It can be more efficient than grid search, especially when some hyperparameters are more important than others.   \n",
        "\n",
        "* Bayesian Optimization: Bayesian optimization uses a probabilistic model to predict the performance of different hyperparameter settings and focuses on exploring promising areas of the hyperparameter space.   \n",
        "\n",
        "* Automated Hyperparameter Tuning Tools: There are tools like Optuna, Hyperopt, and Keras Tuner that automate the hyperparameter tuning process, making it easier to find optimal settings.   \n",
        "\n",
        "The choice of hyperparameter tuning method depends on the complexity of the model, the size of the dataset, and the available computational resources. It's often a good practice to start with manual tuning and then explore more automated methods if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Neural Network Architectures\n",
        "\n",
        "While Multi-Layer Perceptrons (MLPs) are powerful, specialized architectures have emerged to efficiently handle different types of data and tasks. Let's explore some of these architectures and their motivations:\n",
        "\n",
        "* Convolutional Neural Networks (CNNs): CNNs excel at processing images and other grid-like data. They utilize convolutional kernels that act as feature detectors, sliding across the input and extracting local patterns like edges, corners, and textures. This hierarchical feature extraction makes CNNs effective for tasks like image recognition, object detection, and image segmentation.\n",
        "\n",
        "* Recurrent Neural Networks (RNNs): RNNs are designed for sequential data, such as text, time series, and speech. They have recurrent connections that allow them to maintain information about previous inputs, capturing temporal dependencies and context. This memory mechanism is crucial for tasks like language modeling, machine translation, and speech recognition.\n",
        "\n",
        "* Generative Adversarial Networks (GANs): GANs consist of two networks, a generator and a discriminator, engaged in an adversarial training process. The generator tries to create realistic data samples, while the discriminator tries to distinguish between real and generated samples. This competition pushes both networks to improve, leading to the generation of highly realistic data, such as images, videos, and audio.\n",
        "\n",
        "* Autoencoders: Autoencoders are unsupervised learning models that learn to compress and reconstruct input data. They consist of an encoder that compresses the input into a lower-dimensional representation and a decoder that reconstructs the original input from this representation. This compression and reconstruction process forces the network to learn essential features of the input data, leading to effective dimensionality reduction, anomaly detection, and feature learning.\n",
        "\n",
        "* Transformers: Transformers have revolutionized natural language processing tasks. They utilize self-attention mechanisms to capture relationships between different parts of the input sequence, weighing the importance of different parts of the input when processing information. This enables them to capture complex relationships and dependencies, making them effective for tasks like machine translation, text summarization, and question answering.\n",
        "\n",
        "### Core Concepts\n",
        "Despite the differences in architecture, the core concepts we've covered so far—activation functions, backpropagation, and optimization—remain the same across these different types of neural networks.\n",
        "\n",
        "* Activation Functions: Activation functions introduce non-linearity in all these architectures, enabling them to learn complex patterns in their respective data types.\n",
        "\n",
        "* Backpropagation: Backpropagation is used to train all these networks, calculating gradients and updating parameters to minimize the loss function.\n",
        "\n",
        "* Optimization: Optimization algorithms like SGD and Adam are used to optimize the learning process in all these architectures, guiding the networks towards better performance.\n",
        "\n",
        "Understanding these core concepts provides a solid foundation for exploring and understanding more advanced neural network architectures in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Exercise: Building and Training an MLP with PyTorch\n",
        "\n",
        "*   Install PyTorch (if not already installed).\n",
        "*   Find and load a simple dataset (check out [kaggle.com](https://www.kaggle.com/)).\n",
        "*   Building a simple MLP.\n",
        "*   Define the layers (linear, activation functions) and the forward pass.\n",
        "*   Choose a loss function and an optimizer.\n",
        "*   Follow the example above with a basic training loop, forward pass, loss calculation, backpropagation, and optimization.\n",
        "*   Add a visualization to observe the training progress (loss curve).\n",
        "*    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for basic MLP here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimentation and Analysis\n",
        "\n",
        "*   Experiment with different hyperparameters (e.g., number of hidden layers, number of neurons per layer, learning rate, batch size, activation functions). Try one of the strategies for hyperparameter tuning in addition to manual.\n",
        "*   Observe the effects of these changes on the training process and the model's performance.\n",
        "*   Analyze your results (e.g., plot the loss curves, evaluate accuracy on a validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for experimentation and analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
